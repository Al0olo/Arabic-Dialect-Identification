{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95ccee8",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "We can map these short description to its corresponding full description, as EG to be Egypt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "100296d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main libraries \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "from pydantic import BaseModel\n",
    "from fastapi import FastAPI\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "# Our files\n",
    "from data_shuffling_split import *\n",
    "from features_extraction import *\n",
    "from data_preprocess import *\n",
    "from ml_modeling import *\n",
    "from keras_models import *\n",
    "from configs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dbdda2",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "Based on what we got from **Compare ML Models**, we end up by decide of using *Abo Bakr Word2vec model*, alongside with AdaBoost Classifier.\n",
    "\n",
    "**We need to map the label as it numbers into corresponding class delicate do we use LabelEncoder as before**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51373dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AE', 'BH', 'DZ', 'EG', 'IQ', 'JO', 'KW', 'LB', 'LY', 'MA', 'OM',\n",
       "       'PL', 'QA', 'SA', 'SD', 'SY', 'TN', 'YE'], dtype='<U2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_features = 100\n",
    "max_len_str  = 64\n",
    "word2vec_path = \"bakrianoo_unigram_cbow_model/full_uni_cbow_100_twitter.mdl\"\n",
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/\" + word2vec_path)\n",
    "\n",
    "strat_test_set = pd.read_csv(\"dataset/test/strat_test_set.csv\")\n",
    "l_encoder = LabelEncoder()\n",
    "l_encoder.fit(list(strat_test_set[\"dialect\"]))\n",
    "l_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c1c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, choosed_model, word_to_vec_model=word_to_vec_model, \n",
    "                  max_len_str=max_len_str, l_encoder=l_encoder):\n",
    "    '''\n",
    "    The function used to predict the tweets requested by the API.\n",
    "    \n",
    "    Argumen\n",
    "        text              : string, the text we need to classify.\n",
    "        word_to_vec_model : word2vec object, our word embedding model.\n",
    "        max_len_str       : integer, The maximum number of tokens represent each text.\n",
    "        l_encoder         : object, the encode of each class into some label to retrieve the class from.\n",
    "    Return\n",
    "        classifed_text    : dict, the returned classified text into the api request.\n",
    "    '''\n",
    "    # First process the text requested\n",
    "    text_cleaned = clean_text(text)\n",
    "    # Second tokenize that text to get the representation of each token\n",
    "    tokenized_text = tokenize_using_nltk_TreebankWordTokenizer([text_cleaned])\n",
    "    # Retrieve the whole text representation\n",
    "    text_features = text_to_matrix_using_word2vec(word_to_vec_model, tokenized_text, max_len_str)\n",
    "    \n",
    "    # No prediction yet\n",
    "    predicted = ''\n",
    "    # Prediction with chosen ML Model\n",
    "    if choosed_model == \"Machine Learning Model\":\n",
    "        print(\"=====================Machine Model=====================\")\n",
    "        model_path    = \"bakr/AdaBoostClassifier__f1_0.325_ml.sav\"\n",
    "        cls_model     = pickle_load_model(\"models/ml_models/\" + model_path)\n",
    "        \n",
    "        # it return a list with one index to retrieve it directly\n",
    "        predicted = cls_model.predict(text_features)\n",
    "    elif choosed_model == \"Deep Learning Model\":\n",
    "        print(\"=====================Deep Model=====================\")\n",
    "        model_path    = \"models/dl_models/run_with_bakr_word2vec_Adam_lstm_with_batch_learning_rate=0.1__model.h5\"\n",
    "        text_features = text_features.reshape(text_features.shape[0], max_len_str, number_of_features)\n",
    "        dl_model      = keras_load_model(model_path)\n",
    "        predicted     = dl_model.predict(text_features)\n",
    "        predicted     =np.argmax(predicted,axis=1)\n",
    "        \n",
    "    \n",
    "    pred_result = l_encoder.inverse_transform(predicted)\n",
    "    # Return dictionary with request text and the predicted class\n",
    "    classifed_text = {\n",
    "        'query': text,\n",
    "        'We have predict your dialect text as: ': str(pred_result[0])\n",
    "    }\n",
    "    return classifed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44536980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign an instance of the FastAPI class to the variable \"app\" to interact with the api.\n",
    "app = FastAPI(title='Deploying a ML & DL Model with FastAPI')\n",
    "\n",
    "# List available models using Enum\n",
    "class Model(str, Enum):\n",
    "    \"\"\"The class to choose from the models we have, Enum represent fixed value that can not change\"\"\"\n",
    "    ML_Model = \"Machine Learning Model\"\n",
    "    DL_Model = \"Deep Learning Model\"\n",
    "\n",
    "\n",
    "class Text(BaseModel):\n",
    "    \"\"\"\n",
    "    The class used to get the text from the user, \n",
    "    \"\"\"\n",
    "    user_attention: str = \"\"\"Put your Arabic text in empty value of key \"query\" below to predict it and choose which model architecture you need.\"\"\"\n",
    "    query         : str = ''\n",
    "        \n",
    "\n",
    "@app.get(\"/\")\n",
    "def home():\n",
    "    # Once you go to this link you will see the get and post method below to trying out\n",
    "    return \"Congratulations! Your API is working as expected. Now head over to http://localhost:8000/docs.\"\n",
    "\n",
    "\n",
    "# This endpoint handles all the logic necessary for the object detection to work.\n",
    "# It requires the desired model and the dictionary of tweet and default class as we give default values to us\n",
    "# In the api you can try other tweet from some_tweets below\n",
    "@app.post(\"/predict\") \n",
    "def prediction(model: Model, text_dict: Text):\n",
    "    \n",
    "    # Encode the retrived request data \n",
    "    text_dict      = jsonable_encoder(text_dict)\n",
    "    # Run our model\n",
    "    try:\n",
    "        classifed_text = classify_text(text_dict['query'], model)\n",
    "    except:\n",
    "        classifed_text = {\n",
    "        'Error': \"Please Provide us with Arabic text in empty value of key 'query' in the Request body, Thanks ^^.\"\n",
    "        }\n",
    "    return classifed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095039d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [5502]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:42596 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:42596 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:42596 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:42596 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "# Allows the server to be run in this interactive environment\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Host depends on the setup you selected\n",
    "host = \"0.0.0.0\"\n",
    "\n",
    "# uvicorn is fast Asynchronous Server Gateway Interface (ASGI) uvicorn handles the serving\n",
    "# Spin up the server!    \n",
    "uvicorn.run(app, host=host, port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f58cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
