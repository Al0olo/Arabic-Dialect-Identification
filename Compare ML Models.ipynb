{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ddf474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_shuffling_split import *\n",
    "from features_extraction import *\n",
    "from data_preprocess import *\n",
    "from ml_modeling import *\n",
    "from configs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70d68da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_score(model, x_train, y_train, x_val, y_val, x_test, y_test):\n",
    "    \n",
    "    print(\"On Training set\\n\")\n",
    "    f1_score_result(model, x_train, y_train)\n",
    "    print(\"=\"*50)\n",
    "    print(\"On Validation set \\n\")\n",
    "    f1_score_result(model, x_val, y_val)\n",
    "    print(\"=\"*50)\n",
    "    print(\"On Training \\n\")\n",
    "    f1_score_result(model, x_test, y_test)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eb1abf",
   "metadata": {},
   "source": [
    "# Tokenize All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9417f9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the file are:  449033\n",
      "The number of instances in the training data after StratifiedShuffleSplit are:  440052\n",
      "The number of instances in the testing data after StratifiedShuffleSplit are:   8981\n",
      "The number of trainin instances:  440052\n",
      "The number of validation instances:  8981\n",
      "The number of trainin labels :  440052\n",
      "The number of validation labels :  8981\n",
      "Before Tokenization : \n",
      " ['ุญุณุงุจุดุฎุตู ุตุนุจ ูุงู ุงูุดุนุจ ูู ูุงููู ูุณูุณ ูุงูุตููุง ูููู', 'ุญุณุงุจุดุฎุตู ุดู ุบูุฑ ุงูุฌุงุจู ูุตููุง 2018 ููุง ุฒุงูุช ุงูุงููุงุฑ ุงูุบุฑูุจู ูุบูุฑ ุงูุฌุงุจูู ููุฌูุฏู ุงุฐ ุชููุฑูู ูู ุงูุงูุฌุงุจูู ูุงูุณุนุงุฏู ุฎููุง ุงูููุธููู ูุงูููุธูุงุช ูู ูุงุญุฏ ูุฌูุจ ุงุทูุงูู ูุนู ูุชููู ุงุญูู ุณุนุงุฏู ๐ค ', 'ููุงูู ุงูุฒุงุญ ุชุฑุงุก ูุงุนุงุฏ ูููู ุงุญุชูุงู ุจุดุฑุง ูููุจูู ูู ุญุฌุฑ ูุณุงู ูุงูุฑุญููู ูุญุณุณูู ุงูู ูุงููุฑู ุญุงูู ุญุงู ูุนุงู ุงูุณุฎุฑูู ูู ูุถุนู ูููู ุญุงุฌุชู ูุฐููู ุญุณุจู ุงููู ููุนู ุงููููู']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['ุญุณุงุจุดุฎุตู', 'ุตุนุจ', 'ูุงู', 'ุงูุดุนุจ', 'ูู', 'ูุงููู', 'ูุณูุณ', 'ูุงูุตููุง', 'ูููู'], ['ุญุณุงุจุดุฎุตู', 'ุดู', 'ุบูุฑ', 'ุงูุฌุงุจู', 'ูุตููุง', '2018', 'ููุง', 'ุฒุงูุช', 'ุงูุงููุงุฑ', 'ุงูุบุฑูุจู', 'ูุบูุฑ', 'ุงูุฌุงุจูู', 'ููุฌูุฏู', 'ุงุฐ', 'ุชููุฑูู', 'ูู', 'ุงูุงูุฌุงุจูู', 'ูุงูุณุนุงุฏู', 'ุฎููุง', 'ุงูููุธููู', 'ูุงูููุธูุงุช', 'ูู', 'ูุงุญุฏ', 'ูุฌูุจ', 'ุงุทูุงูู', 'ูุนู', 'ูุชููู', 'ุงุญูู', 'ุณุนุงุฏู', '๐ค'], ['ููุงูู', 'ุงูุฒุงุญ', 'ุชุฑุงุก', 'ูุงุนุงุฏ', 'ูููู', 'ุงุญุชูุงู', 'ุจุดุฑุง', 'ูููุจูู', 'ูู', 'ุญุฌุฑ', 'ูุณุงู', 'ูุงูุฑุญููู', 'ูุญุณุณูู', 'ุงูู', 'ูุงููุฑู', 'ุญุงูู', 'ุญุงู', 'ูุนุงู', 'ุงูุณุฎุฑูู', 'ูู', 'ูุถุนู', 'ูููู', 'ุญุงุฌุชู', 'ูุฐููู', 'ุญุณุจู', 'ุงููู', 'ููุนู', 'ุงููููู']]\n",
      "==================================================\n",
      "Before Tokenization : \n",
      " ['ุญุณุงุจุดุฎุตู ูููุงุช ููุง ุจูุฑุช ุงูุจุฑ ู ูููุนู ุงูุซุฑ', 'ุญุณุงุจุดุฎุตู ูุงุงุน . . . ูุจูุงุด ููุชูุถู ููุง ุฏุงู ุจูุงุฏู ููุฌููู ุซููู ุนูู ุงููุนุฏู', 'ุญุณุงุจุดุฎุตู ุณูุนู ุนุฏู ุงููู ููุงู ูุจุณ ุฑุงุจุทููุจ']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['ุญุณุงุจุดุฎุตู', 'ูููุงุช', 'ููุง', 'ุจูุฑุช', 'ุงูุจุฑ', 'ู', 'ูููุนู', 'ุงูุซุฑ'], ['ุญุณุงุจุดุฎุตู', 'ูุงุงุน', '.', '.', '.', 'ูุจูุงุด', 'ููุชูุถู', 'ููุง', 'ุฏุงู', 'ุจูุงุฏู', 'ููุฌููู', 'ุซููู', 'ุนูู', 'ุงููุนุฏู'], ['ุญุณุงุจุดุฎุตู', 'ุณูุนู', 'ุนุฏู', 'ุงููู', 'ููุงู', 'ูุจุณ', 'ุฑุงุจุทููุจ']]\n",
      "Before Tokenization : \n",
      " ['ูุงูุซุฑูู ูู ุฒูุงูู ููุงูููู ูู ููุงู : ูุงูุง ุณูุนูุง ุงููุตุงูุฏ ููููุง ูุง ุชููุฏ ุงูุนุฐุฑ ูุงูุบุฏุฑ ูุงุถุญ ุชุฌูู ุทุฑูู ุงูููุงู : ุงูุง ุญูุธุช ุงูููุงุตู ูุงูุง ุฎุณุฑุช ุงูุฑุตูุฏ', 'ุญุณุงุจุดุฎุตู ุงูุฒูู ุงูุช ุงูุญูู ูุง ุนุฑูุช ุงูุชุณุนูุฑู ุนุดุงู ุชููู ูู go head ุดููู ุชุจู ุงูุณูุงุฑู ุชุณุชูู ุ ', 'ุญุณุงุจุดุฎุตู ูุงุนูุฏููุด ูุทุจุฎ ูู ุญูุด ููู ุฌู ุฑุง ๐ ']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['ูุงูุซุฑูู', 'ูู', 'ุฒูุงูู', 'ููุงูููู', 'ูู', 'ููุงู', ':', 'ูุงูุง', 'ุณูุนูุง', 'ุงููุตุงูุฏ', 'ููููุง', 'ูุง', 'ุชููุฏ', 'ุงูุนุฐุฑ', 'ูุงูุบุฏุฑ', 'ูุงุถุญ', 'ุชุฌูู', 'ุทุฑูู', 'ุงูููุงู', ':', 'ุงูุง', 'ุญูุธุช', 'ุงูููุงุตู', 'ูุงูุง', 'ุฎุณุฑุช', 'ุงูุฑุตูุฏ'], ['ุญุณุงุจุดุฎุตู', 'ุงูุฒูู', 'ุงูุช', 'ุงูุญูู', 'ูุง', 'ุนุฑูุช', 'ุงูุชุณุนูุฑู', 'ุนุดุงู', 'ุชููู', 'ูู', 'go', 'head', 'ุดููู', 'ุชุจู', 'ุงูุณูุงุฑู', 'ุชุณุชูู', 'ุ'], ['ุญุณุงุจุดุฎุตู', 'ูุงุนูุฏููุด', 'ูุทุจุฎ', 'ูู', 'ุญูุด', 'ููู', 'ุฌู', 'ุฑุง', '๐']]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Train and Validation data\n",
    "strat_train_set = read_csv(\"train/strat_train_set.csv\")\n",
    "x_train_text, x_val_text, y_train, y_val = prepare_data(strat_train_set)\n",
    "\n",
    "# Test\n",
    "strat_test_set = pd.read_csv(\"dataset/test/strat_test_set.csv\")\n",
    "x_test_text, y_test = list(strat_test_set['text']), strat_test_set['dialect_l_encoded'].values\n",
    "\n",
    "\n",
    "x_train_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_train_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_train_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "x_val_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_val_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_val_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_val_text_tokenized[:3])\n",
    "\n",
    "\n",
    "x_test_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_test_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_test_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_test_text_tokenized[:3])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d38184",
   "metadata": {},
   "source": [
    "# Abo Bakr Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "348ad964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(440052, 6400)\n",
      "==================================================\n",
      "[-1.892   -0.7603   1.813   -1.012    1.148   -2.803    0.7295   2.883\n",
      "  0.653    2.656   -0.05063 -1.826   -0.656   -0.3872  -2.8     -1.694\n",
      " -1.081    2.838   -1.684   -1.89     0.03732 -1.621   -0.5024   3.002\n",
      "  1.024   -2.393   -1.351    2.922    0.6934  -0.8276  -4.355    0.896\n",
      "  2.215   -0.6357   1.226   -5.832   -0.4255   4.285   -2.77    -2.832\n",
      "  3.697    2.02     0.6646   0.0755   5.61    -2.03    -0.8774   0.2825\n",
      " -1.219    0.10815]\n",
      "(8981, 6400)\n",
      "==================================================\n",
      "[-0.395  -1.198   0.4067 -0.3384 -0.4414  1.207  -0.5366  0.0782  0.333\n",
      "  0.4075 -1.254   0.2238 -0.5186  0.624   1.36    0.8545  0.34   -1.218\n",
      " -3.293  -0.1758 -0.2307  1.958  -1.204   1.968  -0.842  -1.233   0.7466\n",
      " -1.993  -0.4653 -2.162   0.2644 -2.395   2.201   1.48    0.1986  2.328\n",
      "  2.422  -1.893  -0.66   -0.1448  0.5537 -0.3228 -0.4375 -0.5117 -3.07\n",
      " -0.4277  2.225   1.252   0.7383 -1.787 ]\n",
      "(9164, 6400)\n",
      "==================================================\n",
      "[-0.2822  -0.1575   3.271   -5.48    -0.6846   1.536    0.1155   0.1368\n",
      " -2.66    -2.887    0.4595   0.6196  -0.5166   0.4082   0.3364  -1.882\n",
      " -2.713   -2.545    0.4863   0.1699   0.01108  1.388   -2.156    2.8\n",
      " -2.316    1.119   -0.357   -0.5176  -0.3162   0.2896   1.391   -0.1215\n",
      " -0.7866   1.626    2.725   -0.8965   0.9175  -1.507   -1.036   -0.629\n",
      " -0.4058   0.0749   1.287    2.66    -1.376   -1.145    2.055   -0.1971\n",
      " -0.327   -3.309  ]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 100\n",
    "max_len_str = 64\n",
    "\n",
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/bakrianoo_unigram_cbow_model/full_uni_cbow_100_twitter.mdl\")\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_val_text_tokenized, max_len_str)\n",
    "x_test_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_test_text_tokenized, max_len_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cee8b5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Training set\n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.3366397607555471\n",
      "==================================================\n",
      "On Validation set \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.33971718071484247\n",
      "==================================================\n",
      "On Training \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.32802269751200347\n"
     ]
    }
   ],
   "source": [
    "# Test using AdaBoostClassifier \n",
    "\n",
    "model_path    = \"bakr/AdaBoostClassifier__f1_0.325_ml.sav\"\n",
    "model = pickle_load_model(\"models/ml_models/\" + model_path)\n",
    "\n",
    "_ = train_val_test_score(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val, \n",
    "                         x_test_embed_matrix, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "444572b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Training set\n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.4001640715188205\n",
      "==================================================\n",
      "On Validation set \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.40285046208662734\n",
      "==================================================\n",
      "On Training \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.3644696639022261\n"
     ]
    }
   ],
   "source": [
    "# Test using Logistic Regression \n",
    "model_path    = \"bakr/unigram_100d_lg_cls_model_f1_0.366.sav\"\n",
    "model = pickle_load_model(\"models/ml_models/\" + model_path)\n",
    "\n",
    "_ = train_val_test_score(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val, \n",
    "                         x_test_embed_matrix, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75d9519",
   "metadata": {},
   "source": [
    "#  Rezk Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b66f641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(440052, 19200)\n",
      "==================================================\n",
      "[-0.1262   0.2761   0.2466  -0.3464  -0.5044   0.216    0.2651   0.05423\n",
      " -0.3276  -0.2793   0.328    0.1699  -0.05267  0.1941   0.292    0.1654\n",
      " -0.01619 -0.428    0.411    0.0927   0.271    0.6206  -0.04764  0.04465\n",
      "  0.0863   0.06042  0.08374 -0.0927   0.05176 -0.1616  -0.4875   0.4932\n",
      "  0.1333   0.4666   0.0387  -0.19     0.05563 -0.1526   0.549    0.2966\n",
      " -0.0969  -0.345   -0.2896  -0.0667   0.12146  0.2126   0.1146  -0.4404\n",
      " -0.1198   0.2651 ]\n",
      "(8981, 19200)\n",
      "==================================================\n",
      "[-0.1262   0.2761   0.2466  -0.3464  -0.5044   0.216    0.2651   0.05423\n",
      " -0.3276  -0.2793   0.328    0.1699  -0.05267  0.1941   0.292    0.1654\n",
      " -0.01619 -0.428    0.411    0.0927   0.271    0.6206  -0.04764  0.04465\n",
      "  0.0863   0.06042  0.08374 -0.0927   0.05176 -0.1616  -0.4875   0.4932\n",
      "  0.1333   0.4666   0.0387  -0.19     0.05563 -0.1526   0.549    0.2966\n",
      " -0.0969  -0.345   -0.2896  -0.0667   0.12146  0.2126   0.1146  -0.4404\n",
      " -0.1198   0.2651 ]\n",
      "(9164, 19200)\n",
      "==================================================\n",
      "[ 0.629     1.157    -0.6753    0.2048    0.2107    0.516    -0.6455\n",
      "  0.669    -0.72      1.04      0.2937   -0.4473   -0.2487    0.1098\n",
      " -0.1501    0.625     0.012115  0.2568   -1.126     0.2261   -0.08954\n",
      "  0.4692    0.1472    0.4668   -0.0946    0.3938    0.1757    0.413\n",
      " -0.189     0.03577  -0.816     0.8457   -0.3623   -0.564     0.619\n",
      "  0.3655    0.714    -0.3547    0.0713    1.005     0.594     0.671\n",
      "  0.7793    0.1586    0.2812   -0.11224  -0.7334    0.2106    0.4324\n",
      "  0.941   ]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 300\n",
    "max_len_str = 64\n",
    "\n",
    "word2vec_path = \"rezk_unigram_CBOW_model/train_word2vec_cbow__window_3_min_count_300\"\n",
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/\" + word2vec_path)\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_val_text_tokenized, max_len_str)\n",
    "x_test_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_test_text_tokenized, max_len_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffa78f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Training set\n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.5131757155972476\n",
      "==================================================\n",
      "On Validation set \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.5205433693352634\n",
      "==================================================\n",
      "On Training \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.4143387167175906\n"
     ]
    }
   ],
   "source": [
    "# Test using LogisticRegression \n",
    "\n",
    "model_path    = \"rezk/LogisticRegression__f1_0.41_ml.sav\"\n",
    "model = pickle_load_model(\"models/ml_models/\" + model_path)\n",
    "\n",
    "_ = train_val_test_score(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val, \n",
    "                         x_test_embed_matrix, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aadecf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Training set\n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.24181233126994084\n",
      "==================================================\n",
      "On Validation set \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.24562966262108896\n",
      "==================================================\n",
      "On Training \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.22315582714971627\n"
     ]
    }
   ],
   "source": [
    "# Test using GradientBoostingClassifier \n",
    "\n",
    "model_path    = \"rezk/GradientBoostingClassifier__f1_0.22_ml.sav\"\n",
    "model = pickle_load_model(\"models/ml_models/\" + model_path)\n",
    "\n",
    "_ = train_val_test_score(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val, \n",
    "                         x_test_embed_matrix, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6510a990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Training set\n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.3368874587548744\n",
      "==================================================\n",
      "On Validation set \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.34650929740563413\n",
      "==================================================\n",
      "On Training \n",
      "\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.2752073330423396\n"
     ]
    }
   ],
   "source": [
    "# Test using LinearSVC \n",
    "\n",
    "model_path    = \"rezk/LinearSVC__f1_0.279_ml.sav\"\n",
    "model = pickle_load_model(\"models/ml_models/\" + model_path)\n",
    "\n",
    "_ = train_val_test_score(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val, \n",
    "                         x_test_embed_matrix, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2cab71",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "If we compare the model prediction to the human prediction, we may conclude that the task of predict the dialect is semi difficult task for human. So how its if we that compare to the model !.\n",
    "\n",
    "And as in the reference paper the SVC model have over **50%**, but we need more resource to train and test multiple models with either Gridsearch or Zoom out and Zoom in idea.\n",
    "\n",
    "Also I would like to use AdaBoostClassifier trained with AraVec at the end with the API, as if we compared to human prediction of the text, its the best ones as we have very small gap between training and validation, and its doing the same for testing data which never seeing by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e7719a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
