{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9734bf",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "**It required higher resource to train different models and see the effect of each model, so we have used paid server from vast.ai to just display some results and it can be saw in *Compare ML Models* notebook after training.**\n",
    "\n",
    "So I just run some cells from the notebook, But the models we trained on all data are saved in **models direction\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main libraries \n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Our files\n",
    "from configs import *\n",
    "from fetch_data import *\n",
    "from features_extraction import *\n",
    "from data_shuffling_split import *\n",
    "from data_preprocess import *\n",
    "from ml_modeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b125bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set = read_csv(\"train/strat_train_set.csv\")\n",
    "strat_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adb7157",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text, x_val_text, y_train, y_val = prepare_data(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d443a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_train_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_train_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "x_val_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_val_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_val_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_val_text_tokenized[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65ca08",
   "metadata": {},
   "source": [
    "# Curse of Dimensional & sparsity\n",
    "\n",
    "Tasks like **Computer Vision** or **Natural Language Processing** run to problem called **Curse of Dimensional**, and as we have here NLP classification problem, the number of instance are semi-large, but this not the point, the point is what we dealing with is text language, and the language are free of grammar, ritch of vocabulary and others.\n",
    "\n",
    "So to handle like these problems we need to extract features from the text, the old or classical way is using BOW (Bag of Words), and this approach run to the problem of **Curse of Dimensionality** as we will have number of features related to the unique words in our data. Not just that most of these features are zeros, what is we called sparse matrix.\n",
    "\n",
    "Beside of that, this matrix we will get from that approach represent the text not the word itself, so there is no similarity between words and other problem.\n",
    "\n",
    "# Word2Vec\n",
    "\n",
    "From what we have of these problem we moved to another approach related to the word representation.\n",
    "\n",
    "Word2vec is numerical representation of dense vector for the word semantics of meaning, including the implies meaning of the word. So we can use these word representation in our text as we will see.\n",
    "\n",
    "But to train Word2Vec and got a pretty good result of word representation, it first require massive data millions of text document, and second to wait for a while for your model to train. So we use the idea of transfer learning, and use some of the pre trained Arabic Word2Vec models and download it to use in our task. \n",
    "\n",
    "**Check for more information about the models we used:** [AraVec](https://www.sciencedirect.com/science/article/pii/S1877050917321749)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5de1eb",
   "metadata": {},
   "source": [
    "# Build Matrix of Text\n",
    "\n",
    "Any ML or DL model require specific number of features (input) to dealing with, but what we have here with word2vec is word representation. So how it works for text ?\n",
    "\n",
    "We will build a matrix for each text, but we need to limit the number of words in each text, because we can not train the model with different number of words in text.\n",
    "\n",
    "# Note !\n",
    "\n",
    "We can take the maximum number of words in the longest text, but maybe for some documents its has thousand of words, so we use the graph below and other helpful method to get reasonable length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b574d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get how many words inside each text after tokenization\n",
    "num_of_words_in_each_text = [len(text) for text in x_train_text_tokenized]\n",
    "max_len = max(num_of_words_in_each_text)\n",
    "print(\"The max length is: \", max_len)\n",
    "num_of_words_in_each_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3442734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times these value repeated and sort them\n",
    "new_dicts = get_keys_that_val_gr_than_num(num_of_words_in_each_text, 1000)\n",
    "keys = list(new_dicts.keys())\n",
    "values = list(new_dicts.values())\n",
    "plt.style.use('dark_background')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 6)\n",
    "plt.bar(range(len(new_dicts)), values, tick_label=keys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d666b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/bakrianoo_unigram_cbow_model/full_uni_cbow_100_twitter.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = 100\n",
    "max_len_str = 64\n",
    "word2vec_path = \"bakr/\"\n",
    "model_path_to_save = \"models/ml_models/\"\n",
    "estimators = voting_models()\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_val_text_tokenized, max_len_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7fc860",
   "metadata": {},
   "source": [
    "# Train Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty='l2', C=1, multi_class='multinomial', solver='lbfgs', verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d088a7",
   "metadata": {},
   "source": [
    "# Train SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240589b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(C=0.5,  max_iter=50, verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ef9bae",
   "metadata": {},
   "source": [
    "# Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c04c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VotingClassifier(estimators, voting=\"hard\")\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6ab06",
   "metadata": {},
   "source": [
    "# Extr Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56fbb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=100, max_depth=5, max_samples=.1, verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d2aa4",
   "metadata": {},
   "source": [
    "# AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f861c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(C=0.5,  verbose=1)\n",
    "model = AdaBoostClassifier(model,  algorithm=\"SAMME\", n_estimators=5)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693af6be",
   "metadata": {},
   "source": [
    "#  Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc7ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=10, subsample=.1, learning_rate=.5,   max_depth=5, verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0347e6",
   "metadata": {},
   "source": [
    "# XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(max_depth=5, subsample=.1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba93151",
   "metadata": {},
   "source": [
    "# Rezk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_model = load_word2vec_model(\"models/word2vec/rezk_unigram_CBOW_model/train_word2vec_cbow__window_3_min_count_300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5170aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = 300\n",
    "max_len_str = 64\n",
    "word2vec_path = \"rezk/\"\n",
    "model_path_to_save = \"models/ml_models/\"\n",
    "estimators = voting_models()\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(word_to_vec_model, x_val_text_tokenized, max_len_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d496826",
   "metadata": {},
   "source": [
    "# Train Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8d7d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty='l2', C=1, multi_class='multinomial', solver='lbfgs', verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609cb3ea",
   "metadata": {},
   "source": [
    "# Train SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(C=0.5,  max_iter=50, verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad287fd",
   "metadata": {},
   "source": [
    "# Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree_cls = DecisionTreeClassifier(max_depth=5, min_samples_split=2, min_samples_leaf=1)\n",
    "model = BaggingClassifier(base_estimator=dec_tree_cls, n_estimators=100, max_samples=.2, verbose=1)\n",
    "ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c824b240",
   "metadata": {},
   "source": [
    "# Extr Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef2a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=100, max_depth=5, max_samples=.1, verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2fc3aa",
   "metadata": {},
   "source": [
    "# AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(C=0.5,  verbose=1)\n",
    "model = AdaBoostClassifier(model,  algorithm=\"SAMME\", n_estimators=5)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07f3fd",
   "metadata": {},
   "source": [
    "#  Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686488f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=10, subsample=.1, learning_rate=.5,   max_depth=5, verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e40b05",
   "metadata": {},
   "source": [
    "# XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(max_depth=5, subsample=.1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd865d0",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "**We can use different word embedding representation, to see its effect on training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5671d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
